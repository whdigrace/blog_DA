# -*- coding: utf-8 -*-
"""230809 크롤링해온 마지막 이미지 텍스트 분석까지.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ERCFX3v4C1wff38P_sElcVwEd_lOintW
"""

!pip install pymysql

import pymysql
from urllib.parse import quote
from urllib.request import Request,urlopen
import json
import re
import pandas as pd

from bs4 import BeautifulSoup
import requests
import os

def nSearch(word, start, display):
    ci='poHQ5UzGhNFMfZwTsneQ'
    cs='s4cFCwDFSg'
    url = 'https://openapi.naver.com/v1/search/blog.json'
    query=quote(word)
    query_str=f'{url}?query={query}&start={start}&display={display}'
    req = Request(query_str)
    req.add_header("X-Naver-Client-Id",ci)
    req.add_header("X-Naver-Client-Secret",cs)
    resp = urlopen(req)
    data=resp.read()
    jdata=json.loads(data)
    return jdata

def str_filter(text):
    html_spch = ['&quot;','&amp;','&lt;','&gt;','&apos;',
             '&nbsp;','&iexcl;','&cent;','&pound;',
             '&curren;','&yen;','&brvbar;','&sect;',
             '&uml;','&copy;','&ordf;','&laquo;','&not;',
             '&shy;','&reg;','&macr;','&deg;','&plusmn;',
             '&sup2;','&sup3;','&acute;','&micro;','&para;',
             '&middot;','&cedil;','&sup1;','&ordm;','&raquo;',
             '&frac14;','&frac12;','&frac34;','&iquest;']
    html_tag = ['<b>','\n','</b>','<b/>','<a>','</a>','<a/>',
            '<br>','</br>','<br/>','<p>','</p>','<p/>',
            '<strong>','</strong>','<strong/>']
    html_spch_tag = html_spch + html_tag
    or_exp = '|'.join(html_spch_tag)
    text = re.sub(or_exp," ",text)
    return re.sub(r'[^\w\s]',' ',text)

def extract_naverBlog(url):
  e_img_urls=""
  response = requests.get(url)
  soup = BeautifulSoup(response.text, 'html.parser')
  ifra = soup.find('iframe', id='mainFrame')
  post_url = 'https://blog.naver.com' + ifra['src']

  res = requests.get(post_url)
  soup2 = BeautifulSoup(res.text, 'html.parser')

  # 제목 추출
  titles = soup2.find_all('div', {'class': re.compile('^se-module se-module-text se-title-tex.*')})
  post_title = titles[0].text
  post_title = post_title.replace('\n', '')

  # 특수 문자 제거
  post_title=str_filter(post_title)

  #본문 추출
  contents = ''
  txt_contents = soup2.find_all('div', {'class': re.compile('^se-module se-module-tex.*')})
  for p_span in txt_contents:
    for txt in p_span.find_all('span'):
      contents += txt.get_text() + '\n'

  main_text=str_filter(contents)
  main_text=main_text[len(post_title):]

  # 이미지 링크 가져오기
  end_img_urls=[]
  imgs = soup2.find_all('img', class_='se-image-resource')

  for i in [-1]:
    if imgs[i].get('data-lazy-src')==None:
      end_img_urls.append(imgs[i].get('src'))
    else:
      end_img_urls.append(imgs[i].get('data-lazy-src'))

  # php 협찬 사이트
  php_links = soup2.find_all('img', class_='se-inline-image-resource')

  # 네이버 스티커
  Naver_sticker_urls=[]
  Naver_sticker= soup2.find_all('img', class_='se-sticker-image')
  for i in [-1]:
    try:
      Naver_sticker_urls.append(Naver_sticker[i].get('src'))
    except:
      pass

  end_img_urls.extend( Naver_sticker_urls)
  end_img_urls=str(end_img_urls)
  end_img_urls=end_img_urls.replace("'","")
  end_img_urls=end_img_urls.replace('[','')
  end_img_urls=end_img_urls.replace(']','')
  e_img_urls=end_img_urls

  ## url 로 ad 거르기
  ad_words = ['seoulouba', 'mrblog', 'revu', 'dinnerqueen', 'xn--939au0g4vj8sq']
  ad_score=0
  for ad_word in ad_words:
    for php_link in php_links:
      if ad_word in php_link['src']:
        ad_score=1

  for ad_word in ad_words:
    if end_img_urls.find(ad_word)>-1:
      ad_score=1

  return (url, post_title, main_text, e_img_urls, ad_score)

conn = pymysql.connect(host='blogdb.cm2yxwfja9ii.ap-northeast-2.rds.amazonaws.com',
                      user='admin',
                      password='blogdb!2',
                      database='Naver_Blogs',
                      port=3306)
curs=conn.cursor()

# 네이버 api로 가져올 블로그 검색어와 개수( max = 100)
search_word='서면 맛집'
blogs=nSearch(search_word,1,50)

# 네이버 api로 얻은 블로그 url, postdate 데이터베이스에 넣기
# 기본적으로 이미지링크, php url 에 광고 회사 단어 포함시 ad_score = 1
blog_links=[]
for i in blogs['items']:
  blog_links.append(i['link'])
  date=i['postdate']
  link=i['link']
  insert_sql = f"INSERT INTO blogs SET search_word='{search_word}', post_date=DATE_FORMAT(STR_TO_DATE({date}, '%Y%m%d'),'%Y-%m-%d '), link='{link}';"
  curs.execute(insert_sql)
  conn.commit()

# 네이버 api로 얻은 블로그 url 을 이용하여 본문 텍스트, 이미지 업데이트
for blog_url in blog_links:
  blog_info = extract_naverBlog(blog_url)
  title=blog_info[1]
  main_text=blog_info[2]
  i_urls=blog_info[3]
  ad_score=blog_info[4]
  update_sql = f"UPDATE blogs SET title='{title}', main_text='{main_text}', img_urls='{i_urls}', ad_score={ad_score} WHERE link='{blog_url}'"
  curs.execute(update_sql)
  conn.commit()

blog_info = extract_naverBlog('https://blog.naver.com/qhfka8644/223150822095')

!pip install google-cloud-vision

from google.cloud import vision
import io
import os
import requests


# JSON 키 파일로 경로 설정
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/content/project1-393401-6e3fff67ffb5.json'  #본인json파일 삽입
client = vision.ImageAnnotatorClient()


# 이미지 불러오기
image_link='https://storep-phinf.pstatic.net/ogq_5ef4bb3b97ce1/original_22.png?type=p100_100'
image_response = requests.get(image_link)
image_content = image_response.content

image = vision.Image(content=image_content)

# 텍스트 탐지
response = client.text_detection(image=image)
texts = response.text_annotations

# 이미지에서 탐색한 텍스트 특수기호 제거 후 한 문장으로 뽑아내기
import re

img_text= str(texts[0].description).replace('\n',' ')
cleaned_sentence = re.sub(r'[^\w\s]', '', img_text)
print(cleaned_sentence)

import re

img_text= str(texts[0].description).replace('\n',' ')
cleaned_sentence = re.sub(r'[^\w\s]', '', img_text)
print(cleaned_sentence)